![Introduction](./images/1.JPG)


### Ниже представлена архитектура слоев и уравнение `(5)` для вычисления выходного слоя `FullyConnectedLayer`
 
![Page_2](./images/2.JPG)

Каждый слой (`FCLayer` или `ActivationLayer`) принимает входные данные - `X`, и выдает выход - `Y`.
![forard_propagation](./images/forward_propagation.png)
Обратите внимание, что выход каждого слоя - является входом для другого, поэтому в целом процесс `Y = forward_propagation(X)`
выглядит следующим образом
![sequential_forward_propagation](./images/sequential_forward_propagation.png)


### В качестве примера, в нашей нейросети будет использоваться функция ошибки `(6)` - среднеквадратичное отклонение

![Page_3](./images/3.JPG)

Как только входной вектор `X` прошел все трансформации слоев - на выходе имеем вектор `Y`,
от которого считаем функцию ошибки `(6)`, и начиная с этого момента нам нужно как-то обновить веса нейрости (параметры), чтобы 
минимизироват функцию ошибки. Здесь приходит на помощь...
### Метод градиентного спуска

![gradient_descent](./images/gradient_descent.png)

Обратите внимание, что теперь процесс идет в обратную сторону: выход слоев стал входом, а входы, наоборот, выходами
`∇L(X) = backward_propagation(∇L(Y), α)`, см. ниже
![sequential_backward_propagation](./images/sequential_backward_propagation.png)

### Применим теорему о дифференцированиии сложной функции...

![Page_3](./images/4.JPG)
